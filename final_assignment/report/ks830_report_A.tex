\documentclass[10pt, twocolumn]{article}
%% Language and font encodings
\usepackage[british]{babel}

\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm]{geometry}

\setlength{\columnsep}{12pt}

\usepackage{amsmath,amssymb}  % Better maths support & more symbols
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{mathtools}

\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usepackage[utf8]{inputenc} % Any characters can be typed directly from the keyboard, eg ÃÂ©ÃÂ§ÃÂ±
\DeclareUnicodeCharacter{2212}{-}

\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{subcaption}

\usepackage{tabularx}

\usepackage{hyperref}
\urlstyle{same}

% \renewcommand{\cfttoctitlefont}{\fontsize{12}{15}\selectfont\bfseries}
% \renewcommand\cftsecfont{\small}
% \renewcommand\cftsecafterpnum{\vskip 0pt}
% \renewcommand\cftsecpagefont{\small}

\usepackage{pdfsync}  % enable tex source and pdf output synchronicity


\usepackage{fancyhdr}
\fancypagestyle{first}{
	\fancyhf{} % clear all header and footers
	\renewcommand{\headrulewidth}{0pt} % remove the line
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}

}

\fancypagestyle{plain}{
	\fancyhf{} % clear all header and footers
	% \renewcommand{\headrulewidth}{0pt} % remove the line
	% \setlength{\headheight}{5pt}
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}
	\fancyhead[C]{\textbf{\svsubject—\svshortsubject{}\ (ks830)}}
}

\def\svauthor{Kamilė Stankevičiūtė (\texttt{ks830})}
\def\college{Gonville \& Caius College}
\def\svsubject{Data Science: Principles and Practice}
\def\svshortsubject{Final Assignment A}

\usepackage{pdfpages}
\usepackage{float}

% \usepackage{minted}
% \usemintedstyle{colorful}

\begin{document}

\thispagestyle{first}
\pagestyle{plain}
\twocolumn[{
\begin{center}
\LARGE
\textbf{Data Science: Principles and Practice \\ Final Assignment A} \\[4mm]

\large
Kamilė Stankevičiūtė (\texttt{ks830}) \\ Gonville \& Caius College

% \today % October 2019
\end{center} \vskip10mm}]


\section{Data exploration}

I examine the dataset of medical care records of diabetic patients over a period of 10 years (1999-2008), as presented in a given sample \textit{diabetic\_data\_balanced.csv}. This analysis has been done on the subset with at most one record per patient, as described in the next section.

\subsection{Patient demographics}

First I examine patient demographic data. The first part of Figure \ref{genderagecountreadmitted} shows the positively-skewed patient age distribution, where the majority of patients are from 50 to 90 years old.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/age_count.png}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.5\textwidth}
        \centering
		\includegraphics[width=\linewidth]{figures/gender_age_count_readmitted.png}
    \end{subfigure}
	\caption{Patient age distributions categorised by outcome and gender.}\label{genderagecountreadmitted}
\end{figure}
Generally, more adult patients are readmitted than not, and we can observe that patients from the age of 60 have higher occurrence of `<30' label compared to `>30', indicating that they are more likely to be readmitted sooner.

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\linewidth]{figures/gender_age_count.png}
	\caption{Patient age distribution by gender.}\label{genderagecount}
\end{figure}

Further categorising by gender (bottom part of Figure \ref{genderagecountreadmitted}), we can see that the readmission frequency increases earlier for female patients, where the relative occurrence of `<30' and `>30' changes at 60-70 rather than 70-80 years old. In general, we can see that for female patients both the absolute (compared to male) and relative (compared to younger female) admission numbers get higher with increased age, which can be also seen in Figure \ref{genderagecount}. This might suggest the longer lifespan of female patients.

\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/idplots.png}
	\end{subfigure}
	~
	\begin{subfigure}[t]{\textwidth}
        \centering
		\includegraphics[width=\textwidth]{figures/kdeplots.png}
    \end{subfigure}
	\caption{Distributions of admission and discharge identifiers; kernel density estimates for numerical features.}\label{kdeplots}
  \end{figure*}





\subsection{Relationship between the type of treatment and readmission}

For this particular dataset, the kernel density estimate plots seemed to represent most of the important trends of the full scatter plot matrix while being much easier to read (Figure \ref{kdeplots}). The most interesting insights I observed were:

The first peak in admission\_type\_id corresponds to emergency encounters which are very likely to result in readmission within 30 days, since the blue peak there is the highest.

A green peak at id=11 for discharge\_disposition\_id represents the patients who died at the hospital and therefore could never be readmitted. Some discharge disposition types (id=22, id=23) were associated only with patients readmitted within 30 days. Those ids correspond to transfers to another rehabilitation facility and transfers to long term care hospitals.

Patients are likely to be readmitted (the high orange peak) if they come from an emergency department. The first peak for all three densities corresponds to physician referral. The third peak corresponds to missing values, which, given the difference in densities, might indicate bias (patients that did not have source information were more likely to be readmitted).

The length of stay distribution is more skewed to the right for the patients who were readmitted sooner, i.e. the patients which stayed for longer were more likely to be readmitted.

The higher number of diagnoses is associated with the likelihood of readmission as seen from higher densities of orange and blue curves with higher number of diagnoses in the last subplot.


\subsection{Relationship between diabetes conditions and readmission}
The main focus of this dataset is the readmission outcomes for diabetic patients, and their relationship to diabetes-specific treatment.

\section{Machine learning algorithms implementation}

\subsection{Preprocessing}

\paragraph{Anonymisation} Some patients have multiple encounter records (up to 15 per patient). This might skew the results of further analysis (whether exploration or classification) as models might learn to identify particular patients (through patient number or otherwise). For this reason at most one randomly sampled encounter per patient will be included in further analysis, and patient and encounter numbers removed. This leaves 7944 unique instances.

\paragraph{Missing values} The dataset has several features with missing values, most notably weight (97.0\%), payer code (97.6\%), and medical specialty (36.3\%). The first two will be excluded from further analysis. After comparing the numerical feature distributions for missing and non-missing medical specialty values, I will assume that the values are missing at random and replace them (as well as the other missing categorical feature values) with a separate category. There seems to be no numerical feature values missing.

\paragraph{Numerical features} Some features, such as admission source, type, and discharge identifiers should be categorical as those values should not have any ordering associated with them. I will convert those features to one-hot-encoded vectors. Other numerical features will be normalised.

\paragraph{Categorical features} For most categorical features I will be using one-hot encoding. However, to represent the order of age categories, I will encode the age feature as integers.

\paragraph{Feature sets} I will analyse two feature sets: \textit{full} feature set with 198 total features (after preprocessing), and \textit{reduced} feature set with 93 features, which excludes the medication name data and medical specialty. 

\paragraph{Train and test split} I chose 90\%/10\% stratified train/test split with 7149 train and 795 test instances.

\subsection{Simple multi-class classifiers}
Prediction of three readmission outcome labels is an instance of a multi-class classification task. I implement the multi-class \textit{Naive Bayes} (NB), \textit{stochastic gradient descent} (SGD) and \textit{logistic regression}  (LogReg) classifiers (using \textit{one-vs-all} strategy where appropriate).

The results are generally better for the reduced feature set, where 5-fold cross-validation gave the best results for the logistic regression classifier (mean accuracy 53.4\%). All three classifiers performed better than the baseline mean accuracy (random guessing based on label distribution) of 33.3\% (Table \ref{multiclass}). Further grid search on regularisation parameters and regularisation strength increased the mean cross-validation accuracy of logistic regression classifier to 53.9\%.

\begin{table}[]
	\begin{tabularx}{\linewidth}{XXXXX}
		\hline
								 & \textbf{Baseline} & \textbf{NB} & \textbf{LogReg} & \textbf{SGD} \\ \hline
		full   & 33.3\%            & 37.2\%      & 53.6\%          & 50.2\%       \\
		reduced & 33.3\%            & 38.7\%      & 53.8\%          & 52.8\%       \\ \hline
		\end{tabularx}
\caption{Mean training set cross-validation accuracies for selected multi-class classifiers and feature sets.}\label{multiclass}
\end{table}

\paragraph{Kernel trick} Transforming the reduced feature set using the \texttt{RBFSampler} with default parameters resulted in accuracies between 34\% and 37\%, therefore worse predictive power.

\subsection{Ensemble models}




\section{Evaluation}

\section{Dimensionality reduction and embeddings}


\end{document}