\documentclass[10pt, twocolumn]{article}
%% Language and font encodings
\usepackage[british]{babel}

\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm]{geometry}

\setlength{\columnsep}{12pt}

\usepackage{amsmath,amssymb}  % Better maths support & more symbols
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{mathtools}

\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usepackage[utf8]{inputenc} % Any characters can be typed directly from the keyboard, eg ÃÂ©ÃÂ§ÃÂ±
\DeclareUnicodeCharacter{2212}{-}

\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{subcaption}

\usepackage{tabularx}

\usepackage{hyperref}
\urlstyle{same}

% \renewcommand{\cfttoctitlefont}{\fontsize{12}{15}\selectfont\bfseries}
% \renewcommand\cftsecfont{\small}
% \renewcommand\cftsecafterpnum{\vskip 0pt}
% \renewcommand\cftsecpagefont{\small}

\usepackage{pdfsync}  % enable tex source and pdf output synchronicity


\usepackage{fancyhdr}
\fancypagestyle{first}{
	\fancyhf{} % clear all header and footers
	\renewcommand{\headrulewidth}{0pt} % remove the line
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}

}

\fancypagestyle{plain}{
	\fancyhf{} % clear all header and footers
	% \renewcommand{\headrulewidth}{0pt} % remove the line
	% \setlength{\headheight}{5pt}
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}
	\fancyhead[C]{\textbf{\svsubject—\svshortsubject{}\ (ks830)}}
}

\def\svauthor{Kamilė Stankevičiūtė (\texttt{ks830})}
\def\college{Gonville \& Caius College}
\def\svsubject{Data Science: Principles and Practice}
\def\svshortsubject{Final Assignment A}

\usepackage{pdfpages}
\usepackage{float}

% \usepackage{minted}
% \usemintedstyle{colorful}

\begin{document}

\thispagestyle{first}
\pagestyle{plain}
\twocolumn[{
\begin{center}
\LARGE
\textbf{Data Science: Principles and Practice \\ Final Assignment A} \\[4mm]

\large
Kamilė Stankevičiūtė (\texttt{ks830}) \\ Gonville \& Caius College

% \today % October 2019
\end{center} \vskip10mm}]


\section{Data exploration}

I examine the dataset of medical care records of diabetic patients over a period of 10 years (1999-2008), as presented in a given sample \textit{diabetic\_data\_balanced.csv}. This analysis has been done on the subset with at most one record per patient, as described in the next section.

\subsection{Patient demographics}

First I examine patient demographic data. The first part of Figure \ref{genderagecountreadmitted} shows the positively-skewed patient age distribution, where the majority of patients are from 50 to 90 years old.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/age_count.png}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.5\textwidth}
        \centering
		\includegraphics[width=\linewidth]{figures/gender_age_count_readmitted.png}
    \end{subfigure}
	\caption{Patient age distributions categorised by outcome and gender.}\label{genderagecountreadmitted}
\end{figure}
Generally, more adult patients are readmitted than not, and we can observe that patients from the age of 60 have higher occurrence of `<30' label compared to `>30', indicating that they are more likely to be readmitted sooner.

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\linewidth]{figures/gender_age_count.png}
	\caption{Patient age distribution by gender.}\label{genderagecount}
\end{figure}

Further categorising by gender (bottom part of Figure \ref{genderagecountreadmitted}), we can see that the readmission frequency increases earlier for female patients, where the relative occurrence of `<30' and `>30' changes at 60-70 rather than 70-80 years old. In general, we can see that for female patients both the absolute (compared to male) and relative (compared to younger female) admission numbers get higher with increased age, which can be also seen in Figure \ref{genderagecount}. This might suggest the longer lifespan of female patients.

\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/idplots.png}
	\end{subfigure}
	~
	\begin{subfigure}[t]{\textwidth}
        \centering
		\includegraphics[width=\textwidth]{figures/kdeplots.png}
    \end{subfigure}
	\caption{Distributions of admission and discharge identifiers; kernel density estimates for numerical features.}\label{kdeplots}
  \end{figure*}





\subsection{Relationship between the type of treatment and readmission}

For this particular dataset, the kernel density estimate plots seemed to represent most of the important trends of the full scatter plot matrix while being much easier to read (Figure \ref{kdeplots}). The most interesting insights I observed were:

The first peak in admission\_type\_id corresponds to emergency encounters which are very likely to result in readmission within 30 days, since the blue peak there is the highest.

A green peak at id=11 for discharge\_disposition\_id represents the patients who died at the hospital and therefore could never be readmitted. Some discharge disposition types (id=22, id=23) were associated only with patients readmitted within 30 days. Those ids correspond to transfers to another rehabilitation facility and transfers to long term care hospitals.

Patients are likely to be readmitted (the high orange peak) if they come from an emergency department. The first peak for all three densities corresponds to physician referral. The third peak corresponds to missing values, which, given the difference in densities, might indicate bias (patients that did not have source information were more likely to be readmitted).

The length of stay distribution is more skewed to the right for the patients who were readmitted sooner, i.e. the patients which stayed for longer were more likely to be readmitted.

The higher number of diagnoses is associated with the likelihood of readmission as seen from higher densities of orange and blue curves with higher number of diagnoses in the last subplot.


\subsection{Relationship between diabetes conditions and readmission}
The main focus of this dataset is the readmission outcomes for diabetic patients, and their relationship to diabetes-specific treatment.

\LARGE{TODO}

\normalsize

\section{Machine learning algorithms implementation}

\subsection{Preprocessing and design choices}

\paragraph{Anonymisation} Some patients have multiple encounter records (up to 15 per patient). This might skew the results of further analysis (whether exploration or classification) as models might learn to identify particular patients (through patient number or otherwise). For this reason at most one randomly sampled encounter per patient will be included in further analysis, and patient and encounter numbers removed. This leaves 7944 unique instances.

\paragraph{Missing values} The dataset has several features with missing values, most notably weight (97.0\%), payer code (97.6\%), and medical specialty (36.3\%). The first two will be excluded from further analysis. After comparing the numerical feature distributions for missing and non-missing medical specialty values, I will assume that the values are missing at random and replace them (as well as the other missing categorical feature values) with a separate category. There seems to be no numerical feature values missing.

\paragraph{Unknown values} It is possible that some categories present in the testing set are not present in training set, which may cause errors in one-hot-encoding pipeline. I set the parameters of one-hot encoder to ignore such errors, which returns zero- rather than one-hot vector, but the number of features in total stays the same.

\paragraph{Numerical features} Some features, such as admission source, type, and discharge identifiers should be categorical as those values should not have any ordering associated with them. I will convert those features to one-hot-encoded vectors. Other numerical features will be normalised.

\paragraph{Categorical features} For most categorical features I will be using one-hot encoding. However, to represent the relative order of age categories, I will encode the age feature with consecutive integers and normalise them.

\paragraph{Feature sets} I will analyse two feature sets: \textit{full} feature set with 198 total features (after preprocessing), and \textit{reduced} feature set with 93 features, the latter excluding medication and medical specialty data. 

\paragraph{Train and test split} I chose 90\%/10\% stratified train/test split with 7149 train and 795 test instances.

\paragraph{Hyperparameters} I generally tried to use the default hyperparameters. For the best performing classifiers I then used grid search and similar techniques to further improve the accuracy. All hyperparameters used for each classifier can be found in \texttt{training.py} script.

\subsection{Simple multi-class classifiers}
Prediction of three readmission outcome labels is an instance of a multi-class classification task. I implement the multi-class \textit{Naive Bayes} (NB), \textit{stochastic gradient descent} (SGD) and \textit{logistic regression}  (LogReg) classifiers (using \textit{one-vs-all} strategy where appropriate).

\paragraph{Results} The results are generally better for the reduced feature set, where 5-fold cross-validation gave the best results for the logistic regression classifier (mean accuracy 53.4\%). All three classifiers performed better than the baseline mean accuracy (random guessing based on label distribution) of 33.3\% (Table \ref{multiclass}). The reduced feature set generally worked better, although not by much – these results, especially for logistic regression, are likely to be subject to noise.

\paragraph{Grid search} Further grid search on regularisation parameters and regularisation strength increased the mean cross-validation accuracy of logistic regression classifier to 53.9\% for both full and reduced feature sets.

\begin{table}[]
	\begin{tabularx}{\linewidth}{XXXXX}
		\hline
								 & \textbf{Baseline} & \textbf{NB} & \textbf{LogReg} & \textbf{SGD} \\ \hline
		full   & 33.3\%            & 37.2\%      & 53.6\%          & 50.2\%       \\
		red. & 33.3\%            & 38.7\%      & 53.8\%          & 52.8\%       \\ \hline
		\end{tabularx}
\caption{Mean training set cross-validation accuracies for baseline, Naive Bayes, logistic regression and stochastic gradient descent classifiers, trained on full and reduced feature sets.}\label{multiclass}
\end{table}

\paragraph{Kernel trick} Transforming the reduced feature set using the \texttt{RBFSampler} with default parameters resulted in accuracies between 34\% and 37\%, therefore worse predictive power.

\subsection{Ensemble models}
For ensemble models, I have implemented the following: 
\begin{enumerate}[label=(\textit{\roman*})]
	\item A \textit{voting classifier} based on logistic regression, random forest and C-support vector classifiers;
	\item The \textit{bagging} and \textit{pasting} techniques on a \textit{decision tree} classifier (implemented as \textit{random forest});
	\item The \textit{adaptive boosting} (AB) technique on a \textit{decision tree} classifier;
	\item The \textit{gradient boosting} (GB) technique on a decision tree classifier.
\end{enumerate}

\begin{table}[]
	\begin{tabularx}{\linewidth}{XXXXXX}
		\hline
			& \textbf{Voting} & \textbf{Bag.} &\textbf{Past.} & \textbf{AB} & \textbf{GB} \\ \hline
	full   & 54.7\%  & 52.9\% & 53.3\% & 54.1\% & 52.9\% \\
	red. & 54.3\% & 53.7\% & 53.6\% & 53.9\% & 52.3\% \\ 
	\hline
	\end{tabularx}
	\caption{Mean training set cross-validation accuracies for voting, bagging, adaptive boosting and gradient boosting ensembles, trained on full and reduced feature sets. \textit{Out-of-bag scores are reported for the bagging classifier}.}\label{ensemble}
\end{table}

\paragraph{Results} The results are presented in Table \ref{ensemble}, with out-of-bag scores for the bagging classifier as these give a better performance estimate. Generally results are similar between ensemble techniques and feature sets since the differences in accuracies are small and vary depending on the technique chosen. The highest result in this comparison was for the voting classifier which gave the mean cross-validation accuracy of 54.7\% for the full feature set.


\paragraph{Tuning}
Further hyperparameter search using automated machine learning tools boosted the accuracy of the gradient boosting classifier to 54.8\% on the full feature set and \textbf{55.4\% on the reduced feature set}. Since this is the best cross-validation accuracy among all classifiers I will use this model for further evaluation.

\section{Evaluation}


\section{Dimensionality reduction and embeddings}


\end{document}