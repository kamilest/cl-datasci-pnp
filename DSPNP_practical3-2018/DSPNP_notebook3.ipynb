{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Practical Session 3: Deep learning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this practical, we will continue from where the lecture left off and learn more about using Tensorflow. \n\nThe practical will cover a few different network architectures and we will look at different components that are often used in neural networks.\n\nTo start off, let's import tensorflow into our notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Minimal Tensorflow Example"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is the first example from the lectures. We first create a network with two placeholders that adds these together and returns the result. Then, we execute this network with two input values, 4 and 5. This returns the result 9."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\na = tf.placeholder(tf.float32, name=\"a\")\nb = tf.placeholder(tf.float32, name=\"b\")\ny = a + b\n\nwith tf.Session() as sess:\n    result = sess.run(y,\n                      feed_dict={a:4, b:5})\n    print(\"Result: \", result)\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Result:  9.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Occasionally throughout this notebook, the following function will be called:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is necessary to reset the Tensorflow network. We have many different small networks in one notebook and we don't want them interfering with each other, so as a pre-emptive measure we will occasionally reset the computation graph."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training the Parameters"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is the second example from the lecture, showing how to optimize the parameters in your model.\n\nWe define a network that takes a vector x with two features as input, multiplies the features with corresponding parameters in W, and sums them together. We then train this network for 10 epochs over a single training point, optimizing the output towards value 20. Printing out the results, we can see that the output y gradually moves towards the target."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [2], name=\"x\")\ntarget = tf.placeholder(tf.float32, name=\"target\")\nlearning_rate = tf.placeholder(\n    tf.float32,\n    name=\"learning_rate\")\n\nW = tf.get_variable(\"W\", initializer=[0.2, 0.7])\ny = tf.reduce_sum(x * W)\n\nloss = tf.pow(target - y, 2.0)\noptimizer = tf.train.GradientDescentOptimizer(\n    learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(10):\n        result, _ = sess.run(\n            [y, train_op], \n            feed_dict={x: [1.0, 1.0], \n                       target: 20.0, \n                       learning_rate: 0.1}) \n        print(\"Result: \", result)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Result:  0.9\nResult:  8.54\nResult:  13.124001\nResult:  15.874401\nResult:  17.52464\nResult:  18.514786\nResult:  19.108871\nResult:  19.46532\nResult:  19.679192\nResult:  19.807514\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Network Layers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For most cases, we don't actually need to create the trainable variables manually. Instead, the feedfoward layer is available as a pre-defined module."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\ny = tf.layers.dense(x, 1, activation=None)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This creates a hidden layer that takes x as input, has 1 output neuron (we can also create bigger layers of course), and has no non-linear activation. The parameters that connect the two layers together are created automatically and are trained during optimization. By default, these parameters are initialized randomly.\n\nLet's replace the manually created variables with a Tensorflow dense layer."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [None, 2], name=\"x\")\ntarget = tf.placeholder(tf.float32, name=\"target\")\nlearning_rate = tf.placeholder(\n    tf.float32, \n    name=\"learning_rate\")\n\ny = tf.layers.dense(x, 1, activation=None)\n\nloss = tf.pow(target - y, 2.0)\n\noptimizer = tf.train.GradientDescentOptimizer(\n    learning_rate=learning_rate)\n\ntrain_op = optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(10):\n        result, _ = sess.run(\n            [y, train_op], \n            feed_dict={x: [[1.0, 1.0]], \n                       target: 20.0, \n                       learning_rate: 0.1}) \n        print(\"Result: \", result[0][0])\n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Result:  -1.1869861\nResult:  11.5252075\nResult:  16.610083\nResult:  18.644033\nResult:  19.457613\nResult:  19.783047\nResult:  19.913218\nResult:  19.965288\nResult:  19.986115\nResult:  19.994446\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This version actually gets to the correct solution a bit faster than before. That's because it is internally also creating a bias parameter, which adds a bit more power to the model."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "In large networks, you would normally chain together many large layers with non-linear activation functions:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = tf.placeholder(tf.float32, [None, 300], name=\"x\")\nhidden1 = tf.layers.dense(x, 100, activation=tf.tanh)\nhidden2 = tf.layers.dense(hidden1, 50, activation=tf.tanh)\ny = tf.layers.dense(hidden2, 1, activation=tf.sigmoid)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Activation Functions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the last example, we used non-linear activation functions. As we saw in the lectures, this is what gives neural networks their power to model non-linear patterns in the data.\n\nThere are a number of different activation functions to choose from."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The [**sigmoid** function](https://en.wikipedia.org/wiki/Logistic_function), also known as the logistic function, is the most classic non-linear activation. It transforms the value to a range between 0 and 1."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hidden = tf.layers.dense(x, 100, activation=tf.sigmoid)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In modern networks, the [**tanh** function](https://en.wikipedia.org/wiki/Hyperbolic_function) is used more often. It has more flexibility, as it transforms the input value to a range between -1 and 1, and can therefore output negative values as well."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hidden = tf.layers.dense(x, 100, activation=tf.tanh)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another popular one is the <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">**Rectified Linear Unit** function</a>, or the ReLU. This function acts as a linear function above zero, but restricts everything below zero to 0. Doing this, it also introduces a non-linearity."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hidden = tf.layers.dense(x, 100, activation=tf.nn.relu)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The partial linear property of the relu can help it converge faster on some tasks, although in practice I've found tanh to be a more robust option."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) is a special type of activation function. It takes a whole layer as input and converts it into a probability distribution, such that all values are between 0 and 1, and together they sum up to 1. It is often used in the output layers of networks when performing classification, in order to predict a probability distribution over all the possible classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "output = tf.layers.dense(hidden, 2, activation=None)\nprobabilities = tf.nn.softmax(output)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Operations and Useful Functions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tensorflow has corresponding versions of all the main operations you might want to use. This means you can add them into your computation graph and into your neural network."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.abs # absolute value\ntf.negative # computes the negative value\ntf.sign # returns 1, 0 or -1 depending on the sign of the input\ntf.reciprocal # reciprocal 1/x\ntf.square # return input squared\ntf.round # return rounded value\ntf.sqrt # square root\ntf.rsqrt # reciprocal of square root\ntf.pow # power\ntf.exp # exponential",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "<function tensorflow.python.ops.gen_math_ops.exp(x, name=None)>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These operations can be applied to scalar values, but also to vectors, matrices and higher-order tensors. In the latter case, they will be applied element-wise. For example:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\na = tf.placeholder(tf.int32, name=\"a\")\nb = tf.placeholder(tf.float32, [3], name=\"a\")\n\nc = tf.negative(a)\nd = tf.square(b)\n\nwith tf.Session() as sess:\n    c_, d_ = sess.run([c, d], feed_dict={a:4, b:[3.0,2.0,1.0]})\n    print(c_, d_)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-4 [9. 4. 1.]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Some useful operations are performed over a whole vector/matrix tensor and return a single value:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reduce_sum # Add elements together\ntf.reduce_mean # Average over elements\ntf.reduce_min # Minimum value\ntf.reduce_max # Maximum value\ntf.argmax # Index of the largest value\ntf.argmin # Index of the smallest value",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "<function tensorflow.python.ops.math_ops.argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64)>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nb = tf.placeholder(tf.float32, [3,2], name=\"b\")\nc = tf.reduce_sum(b)\n\nwith tf.Session() as sess:\n    c_ = sess.run([c], feed_dict={b:[[6.0, 5.0],[4.0,3.0],[2.0,1.0]]})\n    print(c_)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[21.0]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Different adaptive learning rate strategies are also implemented in Tensorflow as functions. The main ones to try are:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.train.GradientDescentOptimizer\ntf.train.AdadeltaOptimizer\ntf.train.AdamOptimizer",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "tensorflow.python.training.adam.AdamOptimizer"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you are interested in the differences between these strategies, [this blog post](http://ruder.io/optimizing-gradient-descent/) provides more details."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training an XOR Function"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "[XOR](https://en.wikipedia.org/wiki/XOR_gate) is the function that takes two binary values and returns 1 only if one of them is 1 and the other 0, while returning 0 if both of them have the same value.\n\nIt can be a complicated function to optimize and cannot be modeled with a linear model. But let's try anyway."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our dataset consists of all the possible different states that XOR can take:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\ndata_y = [0.0, 1.0, 1.0, 0.0]",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we construct a linear network and optimize it on this dataset, printing out the predictions at each epoch:"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [None, 2], name=\"x\")\ntarget = tf.placeholder(tf.float32, [None], name=\"target\")\nlearning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n\ny = tf.reduce_sum(tf.layers.dense(x, 1, activation=None), axis=1)\n\nloss = tf.reduce_sum(tf.pow(target - y, 2.0))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss)\n\ndata_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\ndata_y = [0.0, 1.0, 1.0, 0.0]\nlr = 0.1\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(50):\n        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n        if epoch % 10 == 0:\n            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch  0 :  0.0\t1.4035221\t1.3642935\t2.7678156\nEpoch  10 :  0.1940817\t0.45410073\t0.44988856\t0.70990753\nEpoch  20 :  0.42001867\t0.4876747\t0.48722243\t0.5548785\nEpoch  30 :  0.47908926\t0.49674278\t0.4966942\t0.51434773\nEpoch  40 :  0.49453297\t0.49914464\t0.49913943\t0.5037511\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see, it's not doing very well. Ideally, the predictions should be \\[0, 1, 1, 0\\], but in this case they are hovering around 0.5 for every input case."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In order to improve this architecture, let's add some non-linear layers into our model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [None, 2], name=\"x\")\ntarget = tf.placeholder(tf.float32, [None], name=\"target\")\nlearning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n\nhidden = tf.layers.dense(x, 5, activation=tf.tanh) # <- non-linear layer\ny = tf.reduce_sum(tf.layers.dense(hidden, 1, activation=tf.sigmoid), axis=1) # <- non-linear layer\n\nloss = tf.reduce_sum(tf.pow(target - y, 2.0))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss)\n\ndata_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\ndata_y = [0.0, 1.0, 1.0, 0.0]\nlr = 1.0\n\ntf.set_random_seed(20)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(50):\n        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n        if epoch % 10 == 0:\n            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))\n",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch  0 :  0.5\t0.28850782\t0.36305997\t0.2056952\nEpoch  10 :  0.28464514\t0.5555107\t0.6265177\t0.42573875\nEpoch  20 :  0.07027962\t0.6488854\t0.7141711\t0.18525329\nEpoch  30 :  0.0814386\t0.8589512\t0.8627182\t0.15936361\nEpoch  40 :  0.060135383\t0.8947844\t0.8951496\t0.118306704\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is much better. The values are much closer to \\[0, 1, 1, 0\\] than before, and they will continue improving if we train for longer.\n\nWe also had to increase the learning rate for this network. It was still learning with the smaller learning rate, but was convering very slowly. As we discussed in the lectures, learning rate is a hyperparameter that can vary quite a bit depending on the network architecture and dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## XOR Classification"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also do classification with Tensorflow. For this, we often use the softmax activation function described above, which predicts the probability for each of the possible classes.\n\nWe also have to change the loss function, as squared error is not suitable for classification. The loss function that works best with softmax is [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy). When minimizing cross entropy, we are essentially minimizing the negative log likelihood of the correct class for each datapoint. That's exactly what we want, as the model learns to assign high values for the correct label.\n\nWe can change the XOR example above to perform classification instead. In this case, we are constructing a binary classifier - choosing between the classes of 0 and 1. When printing the output, we are printing the predicted classes, which were assigned the highest probability by the network."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [None, 2], name=\"x\")\ntarget = tf.placeholder(tf.int32, [None], name=\"target\")\nlearning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n\nhidden = tf.layers.dense(x, 5, activation=tf.tanh)\noutput = tf.layers.dense(hidden, 2, activation=None)\n\nprobabilities = tf.nn.softmax(output)\npredictions = tf.argmax(probabilities, axis=1)\nloss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=target)\nloss = tf.reduce_mean(loss_)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss)\n\ndata_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\ndata_targets = [0, 1, 1, 0]\nlr = 1.0\n\ntf.set_random_seed(20)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(50):\n        result, _ = sess.run([predictions, train_op], feed_dict={x: data_x, target: data_targets, learning_rate: lr})\n        if epoch % 10 == 0:\n            print(\"Epoch \", epoch, \": \", \" \".join([str(x) for x in result]))",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch  0 :  0 1 0 1\nEpoch  10 :  0 1 1 1\nEpoch  20 :  0 1 1 0\nEpoch  30 :  0 1 1 0\nEpoch  40 :  0 1 1 0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As you can see, the model starts off with incorrect predictions, but fairly soon learns to return the correct sequence of \\[0, 1, 1, 0\\]."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Assignment: Classification of House Locations"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the first practical, you used the California House Prices Dataset in order to predict the prices of the houses based on various properties about the houses. In this assignment, we will experiment with Tensorflow and train a model to classify houses based on their ocean proximity."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, we read in the dataset:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndata = pd.read_csv('data/housing.csv')\ndata.head()",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we split the ocean proximity column from the other features and convert the values to numerical IDs. Remember, the ocean_proximity column already contains discrete classes, so it is well-suited for the classification task. However, these are strings and in order to optimize the softmax function in Tensorflow, we need numerical IDs instead of strings. We can use the pandas map function to do the conversion:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = data.copy().drop([\"ocean_proximity\"], axis=1)\nY = data.copy()[\"ocean_proximity\"]\nY = data.copy()[\"ocean_proximity\"].map({\"<1H OCEAN\":0, \"INLAND\":1, \"ISLAND\": 2, \"NEAR BAY\": 3, \"NEAR OCEAN\": 4}).values",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's split off some data for development and testing:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=1)\nX_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, train_size=0.8, random_state=1)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And finally, let's preprocess the input features before giving them to the network. We need to fill in missing values with the imputer, and standardize the values to a similar range using the scaler:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import Imputer \nfrom sklearn import preprocessing\n\nimputer = Imputer(strategy=\"median\")\nimputer.fit(X_train)\n\nX_train = imputer.transform(X_train)\nX_dev = imputer.transform(X_dev)\nX_test = imputer.transform(X_test)\n\nscaler = preprocessing.StandardScaler().fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_dev = scaler.transform(X_dev)\nX_test = scaler.transform(X_test)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now have a dataset that we can work with. \n\nInput features:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(X_train.shape)\nprint(X_dev.shape)\nprint(X_test.shape)\nprint(X_train[:3])",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(13209, 9)\n(3303, 9)\n(4128, 9)\n[[ 0.89872022 -0.89773476 -1.24376976 -0.32171094 -0.69583159 -0.43931447\n  -0.67219421  2.24405982  1.48686249]\n [ 0.68448647 -0.84637731 -0.60683118 -0.17791607 -0.20472295 -0.45260971\n  -0.20959953  0.38636939  0.55799804]\n [ 0.92363112 -0.98177423 -1.16415244 -0.44476616 -0.5808403  -0.63785671\n  -0.54340365  0.65584018  0.37757734]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And the correstponding gold-standard labels:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(y_train.shape)\nprint(y_dev.shape)\nprint(y_test.shape)\nprint(y_train[:10])",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(13209,)\n(3303,)\n(4128,)\n[0 4 0 1 0 0 1 0 4 4]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Based on the code examples above, construct a Tensorflow model, then train, tune and test it on this dataset. Experiment with different model settings and hyperparameters. Calculate and evaluate classification accuracy - the percentage of datapoints where the predicted class matches the gold-standard class.\n\nDuring the practical session, give examples of what you tried and what were your findings."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Some suggestions and tips:\n * The XOR classification code can be a good place to start.\n * The output layer needs to have size 5, because the dataset has 5 possible classes.\n * Try testing on the development set as you are training, to make sure you don't overfit.\n * Evaluate on the dev set as much as you want, but evaluate on the test set only after you have chosen a good set of hyperparameters.\n * You could try different learning rates, hidden layer sizes, learning strategies, etc.\n * Adaptive learning rates can (and sometimes should) be used together with a regular hand-picked learning rate, and different adaptive learning rates can prefer very different regular learning rates."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}