{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Step 1: Uploading and inspecting the data\n",
    "\n",
    "* `head()` for getting a first few rows of the dataset, looking at the kind of values\n",
    "* `info()` for getting the description of the data columns\n",
    "* `value_counts()` for counting the instances of each value in categorical variable (maybe would work for floating-point but results would be rather meaningless).\n",
    "* `describe()` for per-feature summary statistics\n",
    "\n",
    "*Questions on interpretation of the dataset from* `describe()`:\n",
    "1. *Interpreting the values.* Not sure what is meant there. The method just returns per-column (feature) summary statistics.\n",
    "2. *Meaning of percentiles.* Percentiles tell how much the distribution is skewed based on how far away the median is from each of the quartiles—`households` column is one such example where distribution is skewed to the left (or tail-heavy?).\n",
    "3. *Handling of missing values.* Documentation says `NaN` by default, which would be skipped in computing statistics (`skipna=True` by default) but would be kept in arrays and propagate through arithmetic calculations. When all values are `NaN` result in automatic statistic calculation is 0.\n",
    "\n",
    "## Step 2: Splitting the data into training and test sets\n",
    "\n",
    "* `np.random.permutation(len(data))` for shuffling the indices and then taking `test_indices` and `train_indices` from this accordingly;\n",
    "  * `data.iloc[train_indices]` for retrieving the data\n",
    "* `from sklearn.model_selection import train_test_split` for automatically splitting the dataset by describing the fraction of test data\n",
    "* stratified sampling to get similar train and test distributions if some features are imbalanced—both distributions will be more representative\n",
    "* binning to limit the number of strata\n",
    "  * `from sklearn.model_selection import StratifiedShuffleSplit`\n",
    "  * split based on a categorical attribute: `split.split(df, df[\"attr\"])`\n",
    "  \n",
    "## Step 3: Exploring the attributes\n",
    "\n",
    "* Visualisation to \n",
    "  * see most informative attributes \n",
    "  * observe correlations with target label using scatter matrix\n",
    "  * observe the need for normalisation/scaling\n",
    "  * can also indicate the artifacts in the data that would need cleaning up\n",
    "* Create extra features\n",
    "\n",
    "## Step 4: Data preparation and transformations of machine learning algorithms\n",
    "\n",
    "Required for:\n",
    "* handling missing values (if any)\n",
    "  * `from sklearn.impute import SimpleImputer`, `imputer.fit(df)`, `imputer.transform`\n",
    "* converting attribute values into numerical format (e.g. transform categorical data into numbers)—*is this actually needed? and it doesn't really make sense for a regression task anyway, especially if there is no 'increasing' ordering to categories*\n",
    "  * numeric value makes the least sense but one-hot encoding using `sklearn.preprocessing.OneHotEncoder`+`sklearn.preprocessing.LabelBinarizer` is better\n",
    "* scaling or normalising data\n",
    "* data transformers and pipelines for connecting transformations together with `fit`/`transform`/`fit_transform`\n",
    "* `FeatureUnion` for connecting together several pipelines (one for each type of feature of categorical/numerical\n",
    "\n",
    "\n",
    "## Step 5: Implementation, evaluation and fine-tuning of a regression model\n",
    "\n",
    "* training, testing, cross-validation—the usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Experimenting with different steps in the ML pipeline\n",
    "\n",
    "First implement the base of the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def load_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_data(\"housing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratifying dataset\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation and transformations\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # inplace=False\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "def add_features(data):\n",
    "    # add the transformed features that you found useful before\n",
    "    data[\"rooms_per_household\"] = data[\"total_rooms\"] / data[\"households\"]\n",
    "    data[\"bedrooms_per_household\"] = data[\"total_bedrooms\"] / data[\"households\"]\n",
    "    data[\"bedrooms_per_rooms\"] = data[\"total_bedrooms\"] / data[\"total_rooms\"]\n",
    "    data[\"population_per_household\"] = data[\"population\"] / data[\"households\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping less informative features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using alternative preprocessing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling textual and categorical attributes\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer(sparse_output=True)\n",
    "housing_cat_1hot = encoder.fit_transform(housing[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation\n",
    "from sklearn.base import TransformerMixin # TransformerMixin allows you to use fit_transform method\n",
    "from sklearn.base import BaseEstimator\n",
    "# BaseEstimator allows you to drop *args and **kwargs from you constructor\n",
    "# and, in addition, allows you to use methods set_params() and get_params()\n",
    "\n",
    "\n",
    "class CustomLabelBinarizer(TransformerMixin):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = LabelBinarizer(*args, **kwargs)\n",
    "    def fit(self, X, y=0):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "    def transform(self, X, y=0):\n",
    "        return self.encoder.transform(X)\n",
    "    \n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "    \n",
    "rooms_id, bedrooms_id, population_id, household_id = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_rooms = True): # note no *args and **kwargs used this time\n",
    "        self.add_bedrooms_per_rooms = add_bedrooms_per_rooms\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_id] / X[:, household_id]\n",
    "        bedrooms_per_household = X[:, bedrooms_id] / X[:, household_id]\n",
    "        population_per_household = X[:, population_id] / X[:, household_id]\n",
    "        if self.add_bedrooms_per_rooms:\n",
    "            bedrooms_per_rooms = X[:, bedrooms_id] / X[:, rooms_id]\n",
    "            return np.c_[X, rooms_per_household, bedrooms_per_household, \n",
    "                         population_per_household, bedrooms_per_rooms]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, bedrooms_per_household, \n",
    "                         population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder()\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "housing_extra_attribs = pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+\n",
    "                                     [\"rooms_per_household\", \"bedrooms_per_household\", \n",
    "                                      \"population_per_household\", \"bedrooms_per_rooms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "housing_tr_scaled = scaler.fit_transform(housing_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('label_binarizer', CustomLabelBinarizer()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# RMSE\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "def analyse_cv(model):   \n",
    "    scores = cross_val_score(model, housing_prepared, housing_labels,\n",
    "                             scoring = \"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    # cross-validation expects utility function (greater is better)\n",
    "    # rather than cost function (lower is better), so the scores returned\n",
    "    # are negative as they are the opposite of MSE\n",
    "    sqrt_scores = np.sqrt(-scores) \n",
    "    print(\"Scores:\", sqrt_scores)\n",
    "    print(\"Mean:\", sqrt_scores.mean())\n",
    "    print(\"Standard deviation:\", sqrt_scores.std())\n",
    "    \n",
    "analyse_cv(tree_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify the range of hyperparameter values for the grid search to try out \n",
    "param_grid = {'n_estimators': [3, 10, 30, 50], 'max_features': [2, 4, 6, 8]}\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                          scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting feature importance weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "extra_attribs = ['rooms_per_household', 'bedrooms_per_household', 'population_per_household', 'bedrooms_per_rooms']\n",
    "cat_one_hot_attribs = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating best model\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance of simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Bike sharing dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
