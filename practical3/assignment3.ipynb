{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Ensemble Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes and questions\n",
    "\n",
    "### Bagging and pasting\n",
    "The errors of classifiers should be uncorrelated, and one way to ensure this is to train the classifiers on different subsets of data. The `bootstrap` parameter indicates whether bagging or pasting is used.\n",
    "* *Bagging* samples with replacement (so some classifiers might get the same example) (`bootstrap=True`)\n",
    "* *Pasting* samples without replacement (so each classifier gets unique examples, `bootstrap=False`)\n",
    "\n",
    "**Question 1.** The *statistical mode* corresponds to the hard voting strategy where the most frequent prediction is chosed independent of each predictor's confidence.\n",
    "\n",
    "**Question 2.** Since `DecisionTreeClassifier` has the `predict_proba()` method, `BaggingClassifier` will automatically perform the soft voting with weighted probabilities.\n",
    "\n",
    "### Out-of-bag evaluation\n",
    "Due to the nature of random sampling with replacement, it is possible that some instances will be sampled several times and others would not be sampled at all. The ratio approaches $1-\\exp(-1)$, so that around $37\\%$ are *out-of-bag* instances, which can be used as test data.\n",
    "\n",
    "### Random forests\n",
    "* ensembles of decision trees with bagging\n",
    "* roughly equivalent to `BaggingClassifier` with `DecisionTreeClassifier` as base\n",
    "* introduces extra randomness compared to bagging classifier + decision trees by looking for a best feature in a random subset of features rather than considering all features at once\n",
    "    * intuitively adds robustness and diversity in decision trees/predictions\n",
    "    \n",
    "### Feature importance\n",
    "* rank features based on how much they reduce the impurity of all nodes on average across all the decision trees\n",
    "\n",
    "**Question 3.** The feature importances of the `iris` dataset suggest that the petal length and width are more important features that discriminate the examples the best. This corresponds to the previous practical where petal length and width could linearly separate the species better than sepal length and width. The `digits` plot shows which pixels were the most important in discriminating the digit examples. It makes sense that the left and right edges where no digits are written are not important and the important pixels are next to the centre (especially those which are generally filled for some numbers but not others). For example, the center pixel might be important because it distinguishes 0 quite well (hole in the middle).\n",
    "\n",
    "### AdaBoost\n",
    "* The improvement on how classifiers are combined in that the subsequent classifier is more focused on the *errors* on the previous classifiers than the correct ones (but is trained on all of them)\n",
    "* This makes each classifier in the sequence make different types of errors which in the end should cancel each other out. \n",
    "* (**Question 4.**) On the other hand, this slows down the performance as the weights cannot be computed in parallel (they depend on the classifier's performance).\n",
    "* SAMME (stagewise additive modelling with multiclass exponential loss) is another strategy improvement which makes use of class probabilities as well as predictions (when `predict_proba` is available)\n",
    "\n",
    "\n",
    "### Gradient boosting\n",
    "* Difference from AdaBoost is that the classifier is trained on *residual errors* only (not on the full instance).\n",
    "* **Question 5.** `learning_rate` tells how much to shrink each subsequent estimator's contribution by. If each estimator's contribution is small, we use more estimators; if each estimator contributes by a lot, less estimators are used. On the plots comparing the number of estimators, we can see that with lower learning rate the steps in the red decision boundary are much smaller than when the learning rate is high.\n",
    "\n",
    "### Gradient boosting with early stopping\n",
    "* Important not to overfit by running too many estimators and having a less generalisable boundary.\n",
    "* This is done by stopping training as soon as validation set reaches threshold accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the techniques to the `digits` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits[\"data\"], digits[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2)\n",
    "split.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42, solver='warn',\n",
       "                                                 tol=0.0001, verbose=0,\n",
       "                                                 warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     ma...\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma='auto_deprecated',\n",
       "                                  kernel='rbf', max_iter=-1, probability=False,\n",
       "                                  random_state=42, shrinking=True, tol=0.001,\n",
       "                                  verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9472222222222222\n",
      "RandomForestClassifier 0.9638888888888889\n",
      "SVC 0.6027777777777777\n",
      "VotingClassifier 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the baseline as in previous practical—the baseline is just guessing a random digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09297836817653891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class NotXClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.random.randint(low=0, high=11, size=(len(X), 1))\n",
    "\n",
    "not_digit_clf = NotXClassifier()\n",
    "not_digit_clf_scores = []\n",
    "for i in digits.target_names:\n",
    "  score = cross_val_score(not_digit_clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "  not_digit_clf_scores.append(score)\n",
    "\n",
    "print(np.mean(not_digit_clf_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the 10% accuracy baseline, the final performance of `VotingClassifier` is pretty good, although most of the contribution seems to be from the `RandomForestClassifier` and `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9472222222222222\n",
      "RandomForestClassifier 0.8861111111111111\n",
      "VotingClassifier 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "alt_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf)],\n",
    "    voting='hard')\n",
    "alt_voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, alt_voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, since the overall performance dropped without the SVM, it was probably useful in some ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use random forests directly as they are equivalent to the `BaggingClassifier` with `DecisionTreeClassifier` base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=16,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=True, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, \n",
    "                                 n_jobs=-1, random_state=42, oob_score=True)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8861111111111111\n",
      "oob score 0.9206680584551148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "print('accuracy {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('oob score {}'.format(rnd_clf.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs worse than the voting classifiers above. With additional out-of-bag evaluation, it seems that it *generalises quite well* though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting feature importances for the AdaBoost classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a21d0bc90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(ada_clf.feature_importances_.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8111111111111111\n"
     ]
    }
   ],
   "source": [
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the voting and bagging classifiers, the boosted classifier performs worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Applying the three classifiers in the same way as in the practical 3 notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_reg1 = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "tree_reg2.fit(X_train, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "tree_reg3.fit(X_train, y3)\n",
    "\n",
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "print(accuracy_score(y_pred, y_test.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=42, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gbrt = GradientBoostingClassifier(max_depth=10, n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8861111111111111\n",
      "1.9194444444444445\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(gbrt.predict(X_test), y_test))\n",
    "print(mean_squared_error(gbrt.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "gbrt_es = GradientBoostingClassifier(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt_es.n_estimators = n_estimators\n",
    "    gbrt_es.fit(X_train, y_train)\n",
    "    y_pred = gbrt_es.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping\n",
    "\n",
    "print(gbrt_es.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9194444444444444\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(gbrt_es.predict(X_test), y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
