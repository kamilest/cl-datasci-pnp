\documentclass[10pt, twocolumn]{article}
%% Language and font encodings
\usepackage[british]{babel}

\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm]{geometry}

\setlength{\columnsep}{12pt}

\usepackage{amsmath,amssymb}  % Better maths support & more symbols
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{mathtools}

\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usepackage[utf8]{inputenc} % Any characters can be typed directly from the keyboard, eg ÃÂ©ÃÂ§ÃÂ±
\DeclareUnicodeCharacter{2212}{-}

\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{subcaption}

\usepackage{tabularx}

\usepackage{hyperref}
\urlstyle{same}

% \renewcommand{\cfttoctitlefont}{\fontsize{12}{15}\selectfont\bfseries}
% \renewcommand\cftsecfont{\small}
% \renewcommand\cftsecafterpnum{\vskip 0pt}
% \renewcommand\cftsecpagefont{\small}

\usepackage{pdfsync}  % enable tex source and pdf output synchronicity


\usepackage{fancyhdr}
\fancypagestyle{first}{
	\fancyhf{} % clear all header and footers
	\renewcommand{\headrulewidth}{0pt} % remove the line
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}

}

\fancypagestyle{plain}{
	\fancyhf{} % clear all header and footers
	% \renewcommand{\headrulewidth}{0pt} % remove the line
	% \setlength{\headheight}{5pt}
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}
	\fancyhead[C]{\textbf{\svsubject—\svshortsubject{}\ (ks830)}}
}

\def\svauthor{Kamilė Stankevičiūtė (\texttt{ks830})}
\def\college{Gonville \& Caius College}
\def\svsubject{Data Science: Principles and Practice}
\def\svshortsubject{Final Assignment B}

\usepackage{pdfpages}
\usepackage{float}
\usepackage{stfloats}

% \usepackage{minted}
% \usemintedstyle{colorful}

\begin{document}

\thispagestyle{first}
\pagestyle{plain}
\twocolumn[{
\begin{center}
\LARGE
\textbf{Data Science: Principles and Practice \\ Final Assignment B} \\[4mm]

\large
Kamilė Stankevičiūtė (\texttt{ks830}) \\ Gonville \& Caius College \\[4mm]

% \today % October 2019
\end{center} \vskip10mm}]


\section{Data preparation}

I use the raw data (\textit{diabetic\_data\_original.csv}) of the same medical care record dataset \cite{strack2014dataset} as in Assignment A. The following section describes the changes in preprocessing steps with respect to the previous part of the assignment.

\paragraph{Filtering}
Due to the nature of the prediction task, only the patients with at least two recorded encounters will be considered. The filtered dataset contains 47021 encounters from 16773 unique patients (with at least two encounters each). This is a much larger number of examples with any individual patient contributing to a relatively small proportion of the dataset (compared to the balanced dataset in Assignment A). This makes overfitting is less likely, which is why in this case I will allow multiple examples per patient. 

\paragraph{Anonymisation} As before, the encounter and patient numbers will be removed for training, but used to generate the \texttt{length\_of\_next\_stay} feature.

\paragraph{Missing values} 

TODO which features in the dataset contain missing values and how much 

TODO The dataset has several features with missing values, most notably weight (?\%), payer code (?\%), and medical specialty (?\%). The first two will be excluded from further analysis. I will assume that the values are missing at random and replace them (as well as the other missing categorical feature values) with a separate category. There seems to be no numerical feature values missing.

\paragraph{Unknown values, numerical and categorical features} No changes in processing compared to Assignment A.

\paragraph{Feature sets} Since the dataset is larger and the models are generally more powerful, I will train on the full feature set only, leaving feature extraction to the model.

\paragraph{Train and test split} 
TODO definitely split patients so that no patients in training leak to test.

TODO proportion of train/test split and the numbers in every set. Consider using a validation set vs cross-validating on the train set 

\paragraph{Hyperparameters} I generally tried to use the default hyperparameters. 

The hyperparameters used for each classifier can be found in \texttt{training.py} script.


\medskip
 
\bibliographystyle{unsrt}
\bibliography{ks830_report_b}

\end{document}