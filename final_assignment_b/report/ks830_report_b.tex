\documentclass[10pt, twocolumn]{article}
%% Language and font encodings
\usepackage[british]{babel}

\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=3cm,left=2cm,right=2cm]{geometry}

\setlength{\columnsep}{12pt}

\usepackage{amsmath,amssymb}  % Better maths support & more symbols
\usepackage{bm}  % Define \bm{} to use bold math fonts
\usepackage{mathtools}

\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usepackage[utf8]{inputenc} % Any characters can be typed directly from the keyboard, eg ÃÂ©ÃÂ§ÃÂ±
\DeclareUnicodeCharacter{2212}{-}

\usepackage{parskip}
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage{subcaption}

\usepackage{tabularx}

\usepackage{hyperref}
\urlstyle{same}

% \renewcommand{\cfttoctitlefont}{\fontsize{12}{15}\selectfont\bfseries}
% \renewcommand\cftsecfont{\small}
% \renewcommand\cftsecafterpnum{\vskip 0pt}
% \renewcommand\cftsecpagefont{\small}

\usepackage{pdfsync}  % enable tex source and pdf output synchronicity


\usepackage{fancyhdr}
\fancypagestyle{first}{
	\fancyhf{} % clear all header and footers
	\renewcommand{\headrulewidth}{0pt} % remove the line
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}

}

\fancypagestyle{plain}{
	\fancyhf{} % clear all header and footers
	% \renewcommand{\headrulewidth}{0pt} % remove the line
	% \setlength{\headheight}{5pt}
	\setlength{\footskip}{50pt}
	\fancyfoot[C]{\thepage}
	\fancyhead[C]{\textbf{\svsubject—\svshortsubject{}\ (ks830)}}
}

\def\svauthor{Kamilė Stankevičiūtė (\texttt{ks830})}
\def\college{Gonville \& Caius College}
\def\svsubject{Data Science: Principles and Practice}
\def\svshortsubject{Final Assignment B}

\usepackage{pdfpages}
\usepackage{float}
\usepackage{stfloats}

% \usepackage{minted}
% \usemintedstyle{colorful}

\begin{document}

\thispagestyle{first}
\pagestyle{plain}
\twocolumn[{
\begin{center}
\LARGE
\textbf{Data Science: Principles and Practice \\ Final Assignment B} \\[4mm]

\large
Kamilė Stankevičiūtė (\texttt{ks830}) \\ Gonville \& Caius College \\[4mm]

? words
\end{center} \vskip10mm}]


\section{Data preparation}

I use the raw data (\textit{diabetic\_data\_original.csv}) of the same medical care record dataset \cite{strack2014dataset} as in Assignment A. The following section describes the changes in preprocessing steps with respect to the previous Assignment.

\subsection{Selection of training instances}
The preprocessing steps below resulted in 16515 training instances.

\paragraph{Filtering}
Only the patients with at least two recorded encounters were considered, since the later encounter is required to generate the label for the earlier one. To ensure independence of all examples in the dataset (corresponding to the assumptions of the probabilistic model described below), only one randomly sampled example per patient was used.

\paragraph{Multiple readmission} Taking a closer look at the dataset has shown that some patients have recorded follow-on visits even in cases when the readmission outcome was `NO' (for example, at least 44 patients have been \textit{not readmitted more than once}). Further investigation would be necessary to determine the reason for this (e.g. human error, admission to another hospital,...) in order to handle it correctly. I decided to not sample from any no-readmission encounters (of which after initial filtering there were 490).

\subsection{Feature preprocessing}

The main advantage of using the deep learning tools is their capacity to select the most useful features without manual feature engineering by the analyst. For this reason, the models will be trained in an end-to-end manner on all features, but with the following considerations.

\paragraph{Anonymisation} As before, the encounter and patient numbers were removed at training stage.

\paragraph{Missing values} In the full dataset, the \textit{weight} feature was missing in 98.6\% of instances, so I excluded it from training. For other features, the missing values were imputed using median or constant strategy for numerical and categorical features respectively.

\paragraph{Numerical and categorical features} As before, the features were converted to numerical or categorical values based on their meaning, and further normalised/one-hot encoded for performance reasons. To avoid a large number of sparse features, the diagnoses were compressed to broader categories (e.g. circulatory, respiratory, etc.)


\section{Machine learning set-up}

\subsection{Train and test set split}
With a view to get a more generalisable model, I split the data into qualitatively different sets using a single-component t-SNE projection (perplexity=30), holding out the top 10\% of values for testing, and another 10\% for validation.

\subsection{Probabilistic model}
I model length of next stay using \textit{zero-truncated Poisson distribution} (ZTP). I truncated the zero because, if the follow-on visit happens, the length of stay must be at least 1, violating the assumptions of the regular Poisson distribution where observations of 0 are possible (on the other hand, I still allow predictions of more than 14 days that were not observed in the dataset but in theory should be possible). The observations $y_i \in \mathbb{N}^+$ therefore follow \[Y_i \sim \mathrm{ZTP}(f_\theta(x_i))\]

with probability mass function 

\begin{equation}
	\mathbb{P}[Y_i = y_i | \theta, x_i] = \frac{f_\theta(x_i)^{y_i}}{(e^{f_\theta(x_i)} - 1)y_i!}.
	\label{eq1}
\end{equation}

\paragraph{Loss function} For $M$ examples, parameters $\theta$ maximising the probability of the dataset (expressed as the product of probabilities in (\ref{eq1}) for each observation, assuming independence) minimise the loss function

\begin{equation}
	\mathcal{L} = \sum\limits_{i=1}^{M} -y_i \log(f_\theta(x_i)) + \log(e^{f_\theta(x_i)} - 1) + \log(y_i!)
\end{equation}

which I will use for training, but additionally taking the average (multiplying $\mathcal{L}$ by constant $1/M$).

% Following the TensorFlow documentation on Poisson loss function,\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/nn/log_poisson_loss}} the training can be optimised computing the inexact version of this loss, omitting the constant $\log(y_i!)$ term.


\section{Feedforward neural networks}
Ideally, I wish to be able to predict the length of the follow-on visit for a patient from as little as a single encounter rather than considering the encounter sequences: the original dataset suggests that in majority (54\%) of cases this is indeed the most information that a hospital would have for a patient. (It would also be unclear how to handle the multiple readmission anomalies for encounter sequences to be correct).

For this task I implement a set of \textit{feedforward neural networks}.

\subsection{Choice of architecture}
Tempting to assume that the deeper the better – increased number of parameters should improve the predictive power of the neural networks.

At the same time, if there is a simple model that is faster to train yet retains the same predictive power, there is no point in using the less efficient model.


% You could just about squeeze in something, but it's not worth it --
% the real point of this practical is "how to I put together a neural
% network, and how does its design affect what it learns?"


\subsection{Results}
TODO table and comparison of results, insights on which model is the best

Deeper networks tended to suffer from the \textit{vanishing gradient} problem, getting stuck with unlucky initialisation (about 20\% of the time). Adding batch normalisation between layers and using ReLU activation function resolved the issue.


\paragraph{Interpretation}
Medical emergencies are unpredictable in nature. 


\subsection{Impact of PCA on learning effectiveness}

Does it speed training? \textbf{Why?}

Pros. Training speed. Removing low-variance components definitely decrease memory, might have a variable effect on predictive power.

Cons. More overfitting on an equivalent model. Therefore should take more care in applying better regularisation and dropout techniques. 
\paragraph{Interpretation}

\section{Evaluation}
TODO Evaluate the predictive power of the best model on the holdout set.


\medskip
 
\bibliographystyle{unsrt}
\bibliography{ks830_report_b}

\end{document}